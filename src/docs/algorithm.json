{
  "mergeSort": {
    "generalDescription": "The runtime of Merge Sort is O(n log n), where n is the number of elements to be sorted. Merge Sort is a divide-and-conquer algorithm that works by recursively splitting an input array into halves until only single-element subarrays remain. It then merges these subarrays in sorted order, resulting in a fully sorted array. The divide-and-conquer approach results in a logarithmic number of recursive calls, where each recursive call operates on a sublist of size n/2. The merging operation, which compares and combines two sorted sublists of size n/2, takes linear time O(n). Therefore, the total runtime of Merge Sort is O(n log n) as the algorithm requires log n recursive calls, and each call performs O(n) work during the merging step. The O(n log n) runtime of Merge Sort makes it an efficient sorting algorithm for large data sets, and it is often preferred over other sorting algorithms, such as bubble sort or selection sort, which have worse worst-case runtime complexities.",
    "algorithmAnalysis": "Conceptually, a merge sort works as follows: Divide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted). Repeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list. In sorting n objects, merge sort has an average and worst-case performance of O(n log n). If the running time of merge sort for a list of length n is T(n), then the recurrence relation T(n) = 2T(n/2) + n follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the n steps taken to merge the resulting two lists).[4] The closed form follows from the master theorem for divide-and-conquer recurrences.",
    "runtimeComplexity": "The runtime of Merge Sort is O(n log n), where n is the number of elements to be sorted. Merge Sort is a divide-and-conquer algorithm that works by recursively splitting an input array into halves until only single-element subarrays remain. It then merges these subarrays in sorted order, resulting in a fully sorted array. The divide-and-conquer approach results in a logarithmic number of recursive calls, where each recursive call operates on a sublist of size n/2. The merging operation, which compares and combines two sorted sublists of size n/2, takes linear time O(n). Therefore, the total runtime of Merge Sort is O(n log n) as the algorithm requires log n recursive calls, and each call performs O(n) work during the merging step. The O(n log n) runtime of Merge Sort makes it an efficient sorting algorithm for large data sets, and it is often preferred over other sorting algorithms, such as bubble sort or selection sort, which have worse worst-case runtime complexities.",
    "SpaceComplexity": "The space complexity of Merge Sort is O(n), where n is the number of elements in the array being sorted. In Merge Sort, the input array is repeatedly divided into two halves until each sub-array contains only one element. Then, the sub-arrays are merged in sorted order. During the merge operation, an additional array is required to hold the merged sub-arrays. The size of this auxiliary array is the same as the size of the original array being sorted. However, after the merge operation is complete, the auxiliary array is no longer needed and can be deallocated. Therefore, the maximum amount of additional space required by Merge Sort is proportional to the size of the original array, giving a space complexity of O(n). Overall, Merge Sort is an efficient algorithm with a good balance between time and space complexity."
  },
  "quickSort": {
    "generalDescription": "Quicksort is an efficient, general-purpose sorting algorithm. Quicksort was developed by British computer scientist Tony Hoare in 1959[1] and published in 1961.[2] It is still a commonly used algorithm for sorting. Overall, it is slightly faster than merge sort and heapsort for randomized data, particularly on larger distributions.[3] Quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. For this reason, it is sometimes called partition-exchange sort.[4] The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting. Quicksort is a comparison sort, meaning that it can sort items of any type for which a 'less-than' relation (formally, a total order) is defined. Most implementations of quicksort are not stable, meaning that the relative order of equal sort items is not preserved.",
    "algorithmAnalysis": "Quicksort is a type of divide and conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms. Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range. After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. Due to its recursive nature, quicksort (like the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array. The steps for in-place quicksort are: If the range has fewer than two elements, return immediately as there is nothing to do. Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped. Otherwise pick a value, called a pivot, that occurs in the range (the precise manner of choosing depends on the partition routine, and can involve randomness). Partition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way. Since at least one instance of the pivot is present, most partition routines ensure that the value that ends up at the point of division is equal to the pivot, and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced).",
    "runtimeComplexity": "Quick sort is a divide-and-conquer sorting algorithm that has an average case time complexity of O(n log n). However, in the worst case, its time complexity can be O(n^2). In quick sort, the algorithm first selects a pivot element from the array (usually the first or last element). The algorithm then partitions the array into two sub-arrays, one containing elements smaller than the pivot and the other containing elements larger than the pivot. This process is repeated recursively on each sub-array until the entire array is sorted. In the average case, the partitioning process divides the array into roughly two equal sub-arrays, which results in a time complexity of O(n log n). However, in the worst case, the partitioning process divides the array into sub-arrays of size 1 and n-1, which results in a time complexity of O(n^2). Overall, quick sort is a very efficient algorithm for sorting arrays and is often used in practice due to its average case time complexity of O(n log n). However, its worst-case time complexity of O(n^2) means that it may not be the best choice for sorting very large or nearly sorted arrays.",
    "SpaceComplexity": "Overall, quick sort has a space complexity that is proportional to the depth of the recursion stack, which can range from O(log n) in the average case to O(n) in the worst case. However, the additional memory usage required for the partitioning process is generally very small and does not significantly affect the overall space complexity of the algorithm."
  },
  "bubbleSort": {
    "generalDescription": "Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the input list element by element, comparing the current element with the one after it, swapping their values if needed. These passes through the list are repeated until no swaps had to be performed during a pass, meaning that the list has become fully sorted. The algorithm, which is a comparison sort, is named for the way the larger elements 'bubble' up to the top of the list. This simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as quicksort, timsort, or merge sort are used by the sorting libraries built into popular programming languages such as Python and Java.",
    "algorithmAnalysis": "Bubble sort has a worst-case and average complexity of O(n^{2}), where  n is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often  (  log ⁡  ) O(nlog n). Even other  (  2 ) O(n^{2}) sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. For this reason, bubble sort is rarely used in practice. Like insertion sort, bubble sort is adaptive, giving it an advantage over algorithms like quicksort. This means that it may outperform those algorithms in cases where the list is already mostly sorted (having a small number of inversions), despite the fact that it has worse average-case time complexity. For example, bubble sort is  (  ) O(n) on a list that is already sorted, while quicksort would still perform its entire  (  log ⁡  ) O(nlog n) sorting process. While any sorting algorithm can be made  (  ) O(n) on a presorted list simply by checking the list before the algorithm runs, improved performance on almost-sorted lists is harder to replicate.",
    "runtimeComplexity": "Bubble sort is a simple sorting algorithm that has a time complexity of O(n^2) in both the worst-case and average-case scenarios. In bubble sort, the algorithm repeatedly iterates through the array, comparing adjacent elements and swapping them if they are in the wrong order. This process is repeated until the entire array is sorted. The worst-case scenario for bubble sort occurs when the input array is in reverse order. In this case, the algorithm must perform n-1 iterations on the first pass, n-2 iterations on the second pass, and so on, resulting in a total of (n-1) + (n-2) + ... + 1 = n(n-1)/2 comparisons and swaps, which is O(n^2). In the average-case scenario, bubble sort still requires n-1 iterations on the first pass, but subsequent passes require fewer iterations as the largest elements become sorted. On average, the algorithm requires roughly n^2/2 iterations, resulting in an average-case time complexity of O(n^2). Bubble sort is generally not used in practice for sorting large arrays, as its time complexity is very inefficient compared to other sorting algorithms, such as quick sort or merge sort. However, it can be useful for sorting small arrays or as a teaching tool for introducing sorting algorithms due to its simplicity.",
    "SpaceComplexity": "Bubble sort has a space complexity of O(1), which means that the amount of extra memory required by the algorithm does not depend on the size of the input array. In bubble sort, the algorithm swaps adjacent elements in the input array in place, without creating any new data structures. As a result, the algorithm does not require any additional memory beyond the space required to store the input array itself. Therefore, bubble sort is a space-efficient algorithm that can be useful when working with very large arrays or in situations where memory usage is a concern. However, its time complexity is O(n^2), which means that it can be very slow for large input arrays and is generally not used in practice for sorting large datasets."
  },
  "insertionSort": {
    "generalDescription": "Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time by comparisons. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages: Simple implementation: Jon Bentley shows a three-line C/C++ version that is five lines when optimized.[1] Efficient for (quite) small data sets, much like other quadratic (i.e., O(n2)) sorting algorithms More efficient in practice than most other simple quadratic algorithms such as selection sort or bubble sort Adaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(kn) when each element in the input is no more than k places away from its sorted position Stable; i.e., does not change the relative order of elements with equal keys In-place; i.e., only requires a constant amount O(1) of additional memory space Online; i.e., can sort a list as it receives it When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.",
    "algorithmAnalysis": "Insertion sort iterates, consuming one input element each repetition, and grows a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain. Sorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.",
    "runtimeComplexity": "Insertion sort is a simple sorting algorithm that has a time complexity of O(n^2) in the worst-case and average-case scenarios. In insertion sort, the algorithm iterates through the array, comparing each element with the elements before it and inserting it into the correct position in the sorted portion of the array. This process is repeated until the entire array is sorted. The worst-case scenario for insertion sort occurs when the input array is in reverse order. In this case, each element must be compared with all of the elements before it, resulting in a total of 1+2+3+...+(n-1) = n(n-1)/2 comparisons and swaps, which is O(n^2). In the average-case scenario, insertion sort can be more efficient than other O(n^2) algorithms such as bubble sort or selection sort, especially for small input sizes. In the average case, the algorithm requires roughly n^2/4 comparisons and swaps, resulting in an average-case time complexity of O(n^2). Although insertion sort is not as efficient as more advanced sorting algorithms such as quicksort or mergesort, it is still widely used in practice for sorting small input sizes or as a subroutine in more advanced algorithms. Additionally, insertion sort has a lower constant factor than other O(n^2) algorithms, making it more efficient for small input sizes.",
    "SpaceComplexity": "Insertion sort has a space complexity of O(1), which means that the amount of extra memory required by the algorithm does not depend on the size of the input array. In insertion sort, the algorithm sorts the input array in place, without creating any new data structures. The only extra memory used by the algorithm is a constant amount of memory used for temporary variables used during the sorting process. Therefore, insertion sort is a space-efficient algorithm that can be useful when working with very large arrays or in situations where memory usage is a concern. However, its time complexity is O(n^2), which means that it can be very slow for large input arrays and is generally not used in practice for sorting large datasets."
  }
}
